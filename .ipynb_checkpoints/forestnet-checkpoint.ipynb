{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from geonet import tiler, mask, raster, dataset, utils\n",
    "from geonet.visualizations import plotImagePair\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_image = \"../../data/_mvp/liski_train_3857_ds.tif\"\n",
    "src_vector = \"../../data/_mvp/liski_forest_verified_l.shp\"\n",
    "tiles_dir = \"../../data/_mvp/forest_train_tiles\"\n",
    "labels_dir = \"../../data/_mvp/forest_train_labels\"\n",
    "mask_dir = \"../../data/_mvp/forest_train_masks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Tiler...\n",
      "Tiler initialized.\n",
      "dest_dir: ../../data/_mvp/forest_train_tiles\n",
      "dest_crs will be inferred from source data.\n",
      "src_tile_size: (1024, 1024)\n",
      "tile size units metric: False\n",
      "Resampling is set to None\n"
     ]
    }
   ],
   "source": [
    "raster_tiler = tiler.RasterTiler(dest_dir=tiles_dir, src_tile_size=(1024, 1024), verbose=True, tile_overlay=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tiling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a3dedb9a774f9bbec0da4204178ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input data...\n",
      "[1, 2, 3, 4]\n",
      "Source CRS: EPSG:3857\n",
      "Destination CRS: EPSG:3857\n",
      "Inputs OK.\n",
      "count of tile bounds 49\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "\n",
      "Tiling complete. Cleaning up...\n",
      "Done. CRS returned for vector tiling.\n"
     ]
    }
   ],
   "source": [
    "raster_bounds_crs = raster_tiler.tile(src_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the tiler...\n",
      "Initialization done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb767f89bf574aa2a9bf8737f6683fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tiles: 49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_tiler = tiler.VectorTiler(dest_dir=labels_dir, verbose=True)\n",
    "vector_tiler.tile(src_vector,\n",
    "                  tile_bounds=raster_tiler.tile_bounds,\n",
    "                  tile_bounds_crs=raster_bounds_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(path, prefix):\n",
    "    files = os.listdir(path)\n",
    "    for index, file in enumerate(files):\n",
    "        os.rename(os.path.join(path, file), os.path.join(path, prefix+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_prefix(\"../../data/_mvp/forest_train_labels/lis/\", \"lis_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "694\n",
      "694\n",
      "694\n"
     ]
    }
   ],
   "source": [
    "images = os.listdir(\"../../data/_mvp/forest_train_tiles\")\n",
    "labels = os.listdir(\"../../data/_mvp/forest_train_labels\")\n",
    "masks = os.listdir(\"../../data/_mvp/forest_train_masks/\")\n",
    "images.sort()\n",
    "labels.sort()\n",
    "masks.sort()\n",
    "print(len(images))\n",
    "print(len(labels))\n",
    "print(len(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vrn_train_3857_4515433_6816489.tif',\n",
       " 'vrn_train_3857_4515433_6830989.tif',\n",
       " 'vrn_train_3857_4515433_6845490.tif',\n",
       " 'vrn_train_3857_4515433_6859990.tif']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vrn_geoms_4515433_6816489.geojson',\n",
       " 'vrn_geoms_4515433_6830989.geojson',\n",
       " 'vrn_geoms_4515433_6845490.geojson',\n",
       " 'vrn_geoms_4515433_6859990.geojson']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vrn_train_3857_4515433_6816489.tif',\n",
       " 'vrn_train_3857_4515433_6830989.tif',\n",
       " 'vrn_train_3857_4515433_6845490.tif',\n",
       " 'vrn_train_3857_4515433_6859990.tif']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[-5:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"../../data/_mvp/forest_train_tiles/.ipynb_checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(images)):\n",
    "    gdf = gpd.read_file(labels_dir + '/' + labels[i])\n",
    "    ref_im = tiles_dir + '/' + images[i]\n",
    "    fp_mask = mask.footprint_mask(gdf, reference_im=ref_im, shape=(1024, 1024), do_transform=\"reference_im\")\n",
    "    #b_mask = mask.boundary_mask(fp_mask, boundary_width=5)\n",
    "    try:\n",
    "        #c_mask = mask.contact_mask(gdf, reference_im=ref_im, contact_spacing=5)\n",
    "        #mask_full = np.dstack((fp_mask, b_mask, c_mask))\n",
    "        mask_im = \"../../data/_mvp/forest_train_masks/\" + images[i]\n",
    "        Image.fromarray(fp_mask).save(mask_im)\n",
    "    except:\n",
    "        os.remove(\"../../data/_mvp/forest_train_labels/\" + labels[i])\n",
    "        os.remove(\"../../data/_mvp/forest_train_tiles/\" + images[i])\n",
    "        print(labels[i] + \" processing failed. Files removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split files on train and test\n",
    "def test_split(train_dir, train_masks_dir, test_dir, test_masks_dir, shape, per=0.3):\n",
    "    k = np.int(np.round(shape * per))\n",
    "    test_idx = random.sample(range(0, shape), k)\n",
    "    file_names = os.listdir(train_dir)\n",
    "    i = 0\n",
    "    for file_name in file_names:\n",
    "        if i in test_idx:\n",
    "            shutil.move(os.path.join(train_dir, file_name), test_dir)\n",
    "            shutil.move(os.path.join(train_masks_dir, file_name), test_masks_dir)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split(\"../../data/_mvp/forest_train_tiles/\", \"../../data/_mvp/forest_train_masks/\",\n",
    "          \"../../data/_mvp/forest_test_tiles/\", \"../../data/_mvp/forest_test_masks/\", shape=706)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add augmentations\n",
    "aug = A.Compose([\n",
    "    #A.Normalize(mean=(0.0095, 0.0087, 0.0078), std=(0.0075, 0.0070, 0.0060)),\n",
    "    A.RandomRotate90(p=0.6),\n",
    "    A.HorizontalFlip(p=0.6),\n",
    "    #ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def preprocess_input(x, mean=[0.187, 0.182, 0.139, 0.215], std=[0.015, 0.035, 0.038, 0.067], input_space=\"RGB\", input_range=[0,1], **kwargs):\n",
    "\n",
    "    if input_space == \"BGR\":\n",
    "        x = x[..., ::-1].copy()\n",
    "\n",
    "    if input_range is not None:\n",
    "        if x.max() > 1 and input_range[1] == 1:\n",
    "            x = x / 5000.0\n",
    "\n",
    "    if mean is not None:\n",
    "        mean = np.array(mean)\n",
    "        x = x - mean\n",
    "\n",
    "    if std is not None:\n",
    "        std = np.array(std)\n",
    "        x = x / std\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.RandomRotate90(p=0.6),\n",
    "        A.Flip(),\n",
    "        A.Lambda(image=preprocess_input),\n",
    "        A.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocess_input),\n",
    "        A.Lambda(image=to_tensor)\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['agro']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.FPN(\n",
    "    encoder_name=ENCODER, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "    in_channels=4,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DVL/glotovaa/projects/data_science/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:2908: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  \"Using lambda is incompatible with multiprocessing. \"\n"
     ]
    }
   ],
   "source": [
    "trainData = dataset.TiledRasterDataset(\"../../data/_mvp/forest_train_tiles\", \"../../data/_mvp/forest_train_masks\", classes=[\"agro\"], preprocessing=get_preprocessing(preprocessing_fn))\n",
    "validData = dataset.TiledRasterDataset(\"../../data/_mvp/forest_test_tiles\", \"../../data/_mvp/forest_test_masks\", classes=[\"agro\"], preprocessing=get_preprocessing(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DVL/glotovaa/projects/data_science/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:2908: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  \"Using lambda is incompatible with multiprocessing. \"\n"
     ]
    }
   ],
   "source": [
    "x_train_file = raster.get_array_from_tiff(\"../../data/_mvp/kursk_train_3857.tif\")\n",
    "x_train_file = np.dstack(x_train_file)\n",
    "y_train_file = raster.get_array_from_tiff(\"../../data/_mvp/kursk_forest_mask.tif\")[0]\n",
    "x_valid_file = raster.get_array_from_tiff(\"../../data/_mvp/vrn_train_3857.tif\")\n",
    "x_valid_file = np.dstack(x_valid_file)\n",
    "y_valid_file = raster.get_array_from_tiff(\"../../data/_mvp/vrn_3857_forest_mask.tif\")[0]\n",
    "trainData = dataset.RasterDataset(x_train_file.astype(int), y_train_file.astype(int), classes=[\"agro\"], tile_size=1024, step=768, augmentation=aug, preprocessing=get_preprocessing(preprocessing_fn))\n",
    "validData = dataset.RasterDataset(x_valid_file.astype(int), y_valid_file.astype(int), classes=[\"agro\"], tile_size=1024, step=768, augmentation=aug, preprocessing=get_preprocessing(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data loaders\n",
    "train_loader = DataLoader(trainData, batch_size=1, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(validData, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.00035),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device = DEVICE\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics,\n",
    "    device = DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPN(\n",
       "  (encoder): ResNetEncoder(\n",
       "    (conv1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): FPNDecoder(\n",
       "    (p5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (p4): FPNBlock(\n",
       "      (skip_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p3): FPNBlock(\n",
       "      (skip_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p2): FPNBlock(\n",
       "      (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (seg_blocks): ModuleList(\n",
       "      (0): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merge): MergeBlock()\n",
       "    (dropout): Dropout2d(p=0.2, inplace=True)\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode=bilinear)\n",
       "    (2): Activation(\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 147/147 [01:39<00:00,  1.47it/s, dice_loss - 0.5657, iou_score - 0.3226]\n",
      "valid: 100%|██████████| 147/147 [00:26<00:00,  5.48it/s, dice_loss - 0.5303, iou_score - 0.3591]\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.43it/s, dice_loss - 0.4512, iou_score - 0.4312]\n",
      "valid: 100%|██████████| 147/147 [00:27<00:00,  5.43it/s, dice_loss - 0.4457, iou_score - 0.4287]\n",
      "\n",
      "Epoch: 2\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.42it/s, dice_loss - 0.3937, iou_score - 0.4882]\n",
      "valid: 100%|██████████| 147/147 [00:27<00:00,  5.43it/s, dice_loss - 0.5466, iou_score - 0.3179]\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.42it/s, dice_loss - 0.3606, iou_score - 0.5193]\n",
      "valid: 100%|██████████| 147/147 [00:27<00:00,  5.43it/s, dice_loss - 0.3275, iou_score - 0.5519]\n",
      "\n",
      "Epoch: 4\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.43it/s, dice_loss - 0.346, iou_score - 0.537]  \n",
      "valid: 100%|██████████| 147/147 [00:26<00:00,  5.45it/s, dice_loss - 0.3542, iou_score - 0.521] \n",
      "\n",
      "Epoch: 5\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.43it/s, dice_loss - 0.3311, iou_score - 0.5505]\n",
      "valid: 100%|██████████| 147/147 [00:27<00:00,  5.43it/s, dice_loss - 0.5172, iou_score - 0.3595]\n",
      "\n",
      "Epoch: 6\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.42it/s, dice_loss - 0.3164, iou_score - 0.5674]\n",
      "valid: 100%|██████████| 147/147 [00:27<00:00,  5.43it/s, dice_loss - 0.6035, iou_score - 0.2742]\n",
      "\n",
      "Epoch: 7\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.43it/s, dice_loss - 0.3723, iou_score - 0.5093]\n",
      "valid: 100%|██████████| 147/147 [00:27<00:00,  5.44it/s, dice_loss - 0.4094, iou_score - 0.4563] \n",
      "\n",
      "Epoch: 8\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.43it/s, dice_loss - 0.3373, iou_score - 0.5432]\n",
      "valid: 100%|██████████| 147/147 [00:27<00:00,  5.43it/s, dice_loss - 0.3453, iou_score - 0.5301]\n",
      "\n",
      "Epoch: 9\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.43it/s, dice_loss - 0.3012, iou_score - 0.585] \n",
      "valid: 100%|██████████| 147/147 [00:27<00:00,  5.43it/s, dice_loss - 0.3655, iou_score - 0.5105]\n",
      "\n",
      "Epoch: 10\n",
      "train: 100%|██████████| 147/147 [01:43<00:00,  1.43it/s, dice_loss - 0.2993, iou_score - 0.587] \n",
      "valid:  58%|█████▊    | 85/147 [00:15<00:11,  5.36it/s, dice_loss - 0.4999, iou_score - 0.3628]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1ece47e17bf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mvalid_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# do something (save model, change lr, etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/data_science/lib/python3.6/site-packages/segmentation_models_pytorch/utils/train.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, dataloader)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;31m# update loss logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mloss_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mloss_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "max_score = 0.65\n",
    "\n",
    "'''checkpoint = torch.load(\"../forestnet/weights/forestnet_chekpoint_v0.1 - 0.7292470525717364.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']'''\n",
    "\n",
    "for i in range(0, 70):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(train_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        best_model = model\n",
    "        torch.save(best_model, '../geonet/weights/forestnet_DS - {}.pth'.format(max_score))\n",
    "        torch.save({\n",
    "            'epoch': i,\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, '../geonet/weights/forestnet_chekpoint_DS - {}.pth'.format(max_score))\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 15:\n",
    "        optimizer.param_groups[0]['lr'] = 5e-4\n",
    "        print('Decrease decoder learning rate to 5e-4!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, '../forestnet/weights/forestnet_chekpoint_v1.2 - {}.pth'.format(max_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = utils.flatten_file(\"../../data/_mvp/kursk_train_3857.tif\")\n",
    "Y_tr = raster.get_array_from_tiff(\"../../data/_mvp/kursk_forest_mask.tif\")[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127972474, 4)\n",
      "(127972474,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape)\n",
    "print(Y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task':'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'histogram_pool_size': 16,\n",
    "    'min_data_in_leaf': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.train(params, lgb_train, num_boost_round=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refit(model, X, Y, chunk = 5000000):\n",
    "    X = flatten_file(X)\n",
    "    Y = raster.get_array_from_tiff(Y)[0].flatten()\n",
    "    steps = np.int(np.floor(X.shape[0] / chunk))\n",
    "    first_step = 0\n",
    "    for i in range(0, steps):\n",
    "        model.refit(X[first_step:(first_step+chunk), :], Y[first_step:(first_step+chunk)])\n",
    "        first_step += chunk\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = refit(gbm, \"../../data/_mvp/vrn_train_3857.tif\", \"../../data/_mvp/vrn_3857_forest_mask.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f890e0002b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm.save_model('../geonet/weights/forest_gbm_v0.1.2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-cdae3ee3d163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'img' is not defined"
     ]
    }
   ],
   "source": [
    "del(img, mas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = raster.get_array_from_tiff(\"../../data/_mvp/vrn_train_3857_ds.tif\")\n",
    "img = np.dstack(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas = raster.get_array_from_tiff(\"../../data/_mvp/vrn_forest_mask_ds.tif\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DVL/glotovaa/projects/data_science/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:2908: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  \"Using lambda is incompatible with multiprocessing. \"\n"
     ]
    }
   ],
   "source": [
    "testData = dataset.RasterDataset(img, mas, classes=['agro'], tile_size=1024, step=768, preprocessing=get_inference_preprocessing(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testData, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = torch.load('../geonet/weights/best/forestnet_v1.0 - 0.6568768178222868.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_predict(model, img, test_loader):\n",
    "    ext_x = np.zeros(shape=(img.shape[0], img.shape[1]), dtype=np.float32)\n",
    "    step = 768\n",
    "    tile_size = 1024\n",
    "    xc = round(img.shape[0] / step) + 1\n",
    "    yc = round(img.shape[1] / step) + 1\n",
    "\n",
    "    i = 0\n",
    "    for batch in test_loader:\n",
    "        m = i % xc\n",
    "        j = i // xc\n",
    "        #x_tensor = torch.from_numpy(batch[0]).unsqueeze(0)\n",
    "        pr_mask = model.predict(batch[0].cuda())\n",
    "        pr_mask = (pr_mask.cpu().numpy().round(decimals=2))\n",
    "\n",
    "            \n",
    "        if (step*m+tile_size) > img.shape[0]:\n",
    "            if (step*j+tile_size) > img.shape[1]:\n",
    "                ext_x[(img.shape[0]-tile_size):img.shape[0], (img.shape[1]-tile_size):img.shape[1]] = np.maximum(ext_x[(img.shape[0]-tile_size):img.shape[0], (img.shape[1]-tile_size):img.shape[1]], pr_mask)\n",
    "            else:\n",
    "                ext_x[(img.shape[0]-tile_size):img.shape[0], step*j:(step*j+tile_size)] = np.maximum(ext_x[(img.shape[0]-tile_size):img.shape[0], step*j:(step*j+tile_size)], pr_mask)\n",
    "        elif (step*j+tile_size) > img.shape[1]:\n",
    "            ext_x[step*m:(step*m+tile_size), (img.shape[1]-tile_size):img.shape[1]] = np.maximum(ext_x[step*m:(step*m+tile_size), (img.shape[1]-tile_size):img.shape[1]], pr_mask)\n",
    "        else:\n",
    "            ext_x[step*m:(step*m+tile_size), step*j:(step*j+tile_size)] = np.maximum(ext_x[step*m:(step*m+tile_size), step*j:(step*j+tile_size)], pr_mask)\n",
    "    \n",
    "        i += 1\n",
    "    \n",
    "    return ext_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.dstack(raster.get_array_from_tiff(\"../../data/_mvp/vrn_train_3857_ds.tif\"))\n",
    "ext_x = cnn_predict(forest_model, data, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.get_raster_from_array(ext_x, \"../mvp/vrn_forest_predicted_ds.tif\", \"../../data/_mvp/vrn_train_3857_ds.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_predict(img):\n",
    "    forest_model = torch.load('../geonet/weights/best/forestnet_v1.0 - 0.6568768178222868.pth')\n",
    "    image = raster.get_array_from_tiff(img)\n",
    "    cnn_img = np.dstack(image)\n",
    "    testData = dataset.RasterDataset(cnn_img.astype(float), cnn_img[0].astype(float), classes=['agro'], tile_size=1024, step=768, preprocessing=get_inference_preprocessing(preprocessing_fn))\n",
    "    test_loader = DataLoader(testData, batch_size=1, shuffle=False, num_workers=1)\n",
    "    gbm = lgb.Booster(model_file=\"../geonet/weights/best/forest_gbm_v0.1.2.txt\")\n",
    "    cnn_pred = cnn_predict(forest_model, cnn_img, test_loader)\n",
    "    gbm_pred = gbm.predict(utils.flatten_file(img))\n",
    "    gbm_pred = np.reshape(gbm_pred, image[0].shape)\n",
    "    preds = np.maximum(cnn_pred, gbm_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = complex_predict(\"../../data/_mvp/vrn_train_3857_ds.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.get_raster_from_array(d, \"../mvp/vrn_forest_predicted_cpx_1.0.tif\", \"../../data/_mvp/vrn_train_3857.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
