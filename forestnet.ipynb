{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import segmentation_models_pytorch as smp\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from geonet import tiler, mask, raster, dataset, utils\n",
    "from geonet.visualizations import plotImagePair\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_image = \"../../data/_mvp/vrn_19_coord.tif\"\n",
    "src_vector = \"../../data/_mvp/vrn_agro_train.shp\"\n",
    "tiles_dir = \"../../data/_mvp/forest2_train_tiles/vrn/\"\n",
    "labels_dir = \"../../data/_mvp/forest_train_labels/vrn/\"\n",
    "mask_dir = \"../../data/_mvp/forest2_train_masks/vrn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Tiler...\n",
      "Tiler initialized.\n",
      "dest_dir: ../../data/_mvp/forest2_train_tiles/vrn/\n",
      "dest_crs will be inferred from source data.\n",
      "src_tile_size: (1024, 1024)\n",
      "tile size units metric: False\n",
      "Resampling is set to None\n"
     ]
    }
   ],
   "source": [
    "raster_tiler = tiler.RasterTiler(dest_dir=tiles_dir, src_tile_size=(1024, 1024), verbose=True, tile_overlay=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning tiling...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22457f4c90e7451fa4f05ab66ba90be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input data...\n",
      "[1, 2, 3, 4, 5, 6]\n",
      "Source CRS: EPSG:3857\n",
      "Destination CRS: EPSG:3857\n",
      "Inputs OK.\n",
      "count of tile bounds 256\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "reading data from window\n",
      "None\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-423431ed0999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraster_bounds_crs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraster_tiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/data_science/geonet/geonet/tiler.py\u001b[0m in \u001b[0;36mtile\u001b[0;34m(self, src, dest_dir, channel_idxs, nodata, alpha, restrict_to_aoi, dest_fname_base, nodata_threshold)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_tile_bounds\u001b[0m \u001b[0;31m# only keep the tile bounds that make it past the nodata threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtile_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtile_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m                 dest_path = self.save_tile(\n\u001b[1;32m    205\u001b[0m                     tile_data, mask, profile, dest_fname_base)\n",
      "\u001b[0;32m~/projects/data_science/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/data_science/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/data_science/geonet/geonet/tiler.py\u001b[0m in \u001b[0;36mtile_generator\u001b[0;34m(self, src, dest_dir, channel_idxs, nodata, alpha, aoi_boundary, restrict_to_aoi)\u001b[0m\n\u001b[1;32m    334\u001b[0m                         \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannel_idxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                         \u001b[0mboundless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                         fill_value=self.nodata)\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                     src_data = self.src.read(\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mrasterio/_base.pyx\u001b[0m in \u001b[0;36mrasterio._base.DatasetBase.__exit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/projects/data_science/lib/python3.6/site-packages/rasterio/env.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting env context: %r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0mdelenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raster_bounds_crs = raster_tiler.tile(src_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the tiler...\n",
      "Initialization done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a347f6da6164b66a68f989d72222eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num tiles: 240\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_tiler = tiler.VectorTiler(dest_dir=labels_dir, verbose=True)\n",
    "vector_tiler.tile(src_vector,\n",
    "                  tile_bounds=raster_tiler.tile_bounds,\n",
    "                  tile_bounds_crs=raster_bounds_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(path, prefix):\n",
    "    files = os.listdir(path)\n",
    "    for index, file in enumerate(files):\n",
    "        os.rename(os.path.join(path, file), os.path.join(path, prefix+file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_prefix(\"../../data/_mvp/forest_train_labels/ostrog/\", \"ostrog_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662\n",
      "707\n",
      "662\n"
     ]
    }
   ],
   "source": [
    "images = os.listdir(\"../../data/_mvp/forest2_train_tiles\")\n",
    "labels = os.listdir(\"../../data/_mvp/forest_train_labels\")\n",
    "masks = os.listdir(\"../../data/_mvp/forest2_train_masks\")\n",
    "images.sort()\n",
    "labels.sort()\n",
    "masks.sort()\n",
    "print(len(images))\n",
    "print(len(labels))\n",
    "print(len(masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kursk_3833411_6695462.tif',\n",
       " 'kursk_3833411_6707884.tif',\n",
       " 'kursk_3833411_6720305.tif',\n",
       " 'kursk_3833411_6732727.tif',\n",
       " 'kursk_3833411_6745148.tif']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'kur_geoms_3833411_6695462.geojson',\n",
       " 'kur_geoms_3833411_6707884.geojson',\n",
       " 'kur_geoms_3833411_6720305.geojson',\n",
       " 'kur_geoms_3833411_6732727.geojson']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'kursk_3833411_6695462.tif',\n",
       " 'kursk_3833411_6707884.tif',\n",
       " 'kursk_3833411_6720305.tif',\n",
       " 'kursk_3833411_6732727.tif']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree(\"../../data/_mvp/forest2_test_tiles/.ipynb_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, filename in enumerate(os.listdir(\"../../data/_mvp/forest2_train_masks/t/\")): \n",
    "    dst = filename.split(\"_\")\n",
    "    del dst[1]\n",
    "    dst = '_'.join(dst)\n",
    "    src =\"../../data/_mvp/forest2_train_masks/t/\" + filename \n",
    "    dst =\"../../data/_mvp/forest2_train_masks/t/\" + dst \n",
    "          \n",
    "    # rename() function will \n",
    "    # rename all the files \n",
    "    os.rename(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(images)):\n",
    "    gdf = gpd.read_file(labels_dir + '/' + labels[i])\n",
    "    ref_im = tiles_dir + '/' + images[i]\n",
    "    fp_mask = mask.footprint_mask(gdf, reference_im=ref_im, shape=(1024, 1024), do_transform=\"reference_im\")\n",
    "    #b_mask = mask.boundary_mask(fp_mask, boundary_width=5)\n",
    "    try:\n",
    "        #c_mask = mask.contact_mask(gdf, reference_im=ref_im, contact_spacing=5)\n",
    "        #mask_full = np.dstack((fp_mask, b_mask, c_mask))\n",
    "        mask_im = \"../../data/_mvp/forest_train_masks/ostrog/\" + images[i]\n",
    "        Image.fromarray(fp_mask).save(mask_im)\n",
    "    except:\n",
    "        os.remove(\"../../data/_mvp/forest_train_labels/ostrog/\" + labels[i])\n",
    "        os.remove(\"../../data/_mvp/forest_train_tiles/ostrog/\" + images[i])\n",
    "        print(labels[i] + \" processing failed. Files removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split files on train and test\n",
    "def test_split(train_dir, train_masks_dir, test_dir, test_masks_dir, shape, per=0.3):\n",
    "    k = np.int(np.round(shape * per))\n",
    "    test_idx = random.sample(range(0, shape), k)\n",
    "    file_names = os.listdir(train_dir)\n",
    "    i = 0\n",
    "    for file_name in file_names:\n",
    "        if i in test_idx:\n",
    "            shutil.move(os.path.join(train_dir, file_name), test_dir)\n",
    "            shutil.move(os.path.join(train_masks_dir, file_name), test_masks_dir)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split(\"../../data/_mvp/forest2_train_tiles/\", \"../../data/_mvp/forest2_train_masks/\",\n",
    "          \"../../data/_mvp/forest2_test_tiles/\", \"../../data/_mvp/forest2_test_masks/\", shape=946)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add augmentations\n",
    "aug = A.Compose([\n",
    "    #A.Normalize(mean=(0.0095, 0.0087, 0.0078), std=(0.0075, 0.0070, 0.0060)),\n",
    "    A.RandomRotate90(p=0.6),\n",
    "    A.HorizontalFlip(p=0.6),\n",
    "    #ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def preprocess_input(x, mean=[0.187, 0.182, 0.139, 0.215, 0, 0], std=[0.015, 0.035, 0.038, 0.067, 1, 1], input_space=\"RGB\", input_range=[0,1], **kwargs):\n",
    "\n",
    "    if input_space == \"BGR\":\n",
    "        x = x[..., ::-1].copy()\n",
    "\n",
    "    if input_range is not None:\n",
    "        if x.max() > 1 and input_range[1] == 1:\n",
    "            x = x / 5000.0\n",
    "\n",
    "    if mean is not None:\n",
    "        mean = np.array(mean)\n",
    "        x = x - mean\n",
    "\n",
    "    if std is not None:\n",
    "        std = np.array(std)\n",
    "        x = x / std\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Flip(),\n",
    "        A.GridDistortion(p=0.4),\n",
    "        #A.Downscale(scale_min=0.2, scale_max=0.3, p=0.3),\n",
    "        A.Transpose(),\n",
    "        A.Lambda(image=preprocess_input),\n",
    "        A.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_preprocessing(preprocessing_fn):\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        A.Lambda(image=preprocess_input),\n",
    "        A.Lambda(image=to_tensor)\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "ENCODER = 'se_resnext50_32x4d'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['agro']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.FPN(\n",
    "    encoder_name=ENCODER, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "    in_channels=6,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DVL/glotovaa/projects/data_science/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:2908: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  \"Using lambda is incompatible with multiprocessing. \"\n"
     ]
    }
   ],
   "source": [
    "trainData = dataset.TiledRasterDataset(\"../../data/_mvp/forest2_train_tiles\", \"../../data/_mvp/forest2_train_masks\", classes=[\"agro\"], preprocessing=get_preprocessing(preprocessing_fn))\n",
    "validData = dataset.TiledRasterDataset(\"../../data/_mvp/forest2_test_tiles\", \"../../data/_mvp/forest2_test_masks\", classes=[\"agro\"], preprocessing=get_preprocessing(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DVL/glotovaa/projects/data_science/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:2908: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  \"Using lambda is incompatible with multiprocessing. \"\n"
     ]
    }
   ],
   "source": [
    "x_train_file = raster.get_array_from_tiff(\"../../data/_mvp/kursk_train_3857.tif\")\n",
    "x_train_file = np.dstack(x_train_file)\n",
    "y_train_file = raster.get_array_from_tiff(\"../../data/_mvp/kursk_forest_mask.tif\")[0]\n",
    "x_valid_file = raster.get_array_from_tiff(\"../../data/_mvp/vrn_train_3857.tif\")\n",
    "x_valid_file = np.dstack(x_valid_file)\n",
    "y_valid_file = raster.get_array_from_tiff(\"../../data/_mvp/vrn_3857_forest_mask.tif\")[0]\n",
    "trainData = dataset.RasterDataset(x_train_file.astype(int), y_train_file.astype(int), classes=[\"agro\"], tile_size=1024, step=768, augmentation=aug, preprocessing=get_preprocessing(preprocessing_fn))\n",
    "validData = dataset.RasterDataset(x_valid_file.astype(int), y_valid_file.astype(int), classes=[\"agro\"], tile_size=1024, step=768, augmentation=aug, preprocessing=get_preprocessing(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-23cc7b0ed107>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# add data loaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainData' is not defined"
     ]
    }
   ],
   "source": [
    "# add data loaders\n",
    "train_loader = DataLoader(trainData, batch_size=1, shuffle=True, num_workers=2)\n",
    "valid_loader = DataLoader(validData, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.utils.losses.BCELoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW([ \n",
    "    dict(params=model.parameters(), lr=0.00015),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device = DEVICE\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics,\n",
    "    device = DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPN(\n",
       "  (encoder): SENetEncoder(\n",
       "    (layer0): Sequential(\n",
       "      (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (pool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    )\n",
       "    (layer1): Sequential(\n",
       "      (0): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (3): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (3): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (4): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (5): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): SEResNeXtBottleneck(\n",
       "        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (se_module): SEModule(\n",
       "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (sigmoid): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): FPNDecoder(\n",
       "    (p5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (p4): FPNBlock(\n",
       "      (skip_conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p3): FPNBlock(\n",
       "      (skip_conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p2): FPNBlock(\n",
       "      (skip_conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (seg_blocks): ModuleList(\n",
       "      (0): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merge): MergeBlock()\n",
       "    (dropout): Dropout2d(p=0.2, inplace=True)\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): UpsamplingBilinear2d(scale_factor=4.0, mode=bilinear)\n",
       "    (2): Activation(\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train:   0%|          | 0/662 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DVL/glotovaa/projects/data_science/lib/python3.6/site-packages/torch/nn/modules/loss.py:529: UserWarning: Using a target size (torch.Size([1, 1024, 1024])) that is different to the input size (torch.Size([1, 1, 1024, 1024])) is deprecated. Please ensure they have the same size.\n",
      "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 662/662 [03:20<00:00,  3.30it/s, bce_loss - 0.2335, iou_score - 0.357]  \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.1972, iou_score - 0.5508]\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.1396, iou_score - 0.4636]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.1092, iou_score - 0.5511] \n",
      "\n",
      "Epoch: 2\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.1188, iou_score - 0.5098]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.1373, iou_score - 0.5473]\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.1049, iou_score - 0.5457]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.73it/s, bce_loss - 0.1348, iou_score - 0.5029]\n",
      "\n",
      "Epoch: 4\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.1002, iou_score - 0.5511] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.73it/s, bce_loss - 0.1159, iou_score - 0.5613] \n",
      "\n",
      "Epoch: 5\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.11, iou_score - 0.538]    \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.73it/s, bce_loss - 0.1115, iou_score - 0.521]  \n",
      "\n",
      "Epoch: 6\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.08977, iou_score - 0.57]  \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.1188, iou_score - 0.5364]\n",
      "\n",
      "Epoch: 7\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.09061, iou_score - 0.5746]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.4422, iou_score - 0.43]  \n",
      "\n",
      "Epoch: 8\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.08876, iou_score - 0.5834]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.72it/s, bce_loss - 0.8116, iou_score - 0.4239]\n",
      "\n",
      "Epoch: 9\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.1116, iou_score - 0.5566]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1839, iou_score - 0.3956]\n",
      "\n",
      "Epoch: 10\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.08939, iou_score - 0.5827]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1492, iou_score - 0.4897]\n",
      "\n",
      "Epoch: 11\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.08489, iou_score - 0.5924]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.72it/s, bce_loss - 0.09526, iou_score - 0.5939]\n",
      "\n",
      "Epoch: 12\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.08143, iou_score - 0.6018]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.1642, iou_score - 0.3693]\n",
      "\n",
      "Epoch: 13\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.08194, iou_score - 0.6024]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.38, iou_score - 0.1263]  \n",
      "\n",
      "Epoch: 14\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.08047, iou_score - 0.6065]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.72it/s, bce_loss - 0.1251, iou_score - 0.5255]\n",
      "\n",
      "Epoch: 15\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07977, iou_score - 0.6104]  \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.08481, iou_score - 0.6311]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 16\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.08564, iou_score - 0.5955]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.71it/s, bce_loss - 0.3052, iou_score - 0.1854]\n",
      "\n",
      "Epoch: 17\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07733, iou_score - 0.6191]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.73it/s, bce_loss - 0.1171, iou_score - 0.5116]\n",
      "\n",
      "Epoch: 18\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07813, iou_score - 0.6164]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.09867, iou_score - 0.5708]\n",
      "\n",
      "Epoch: 19\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07523, iou_score - 0.6288]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.12, iou_score - 0.604]   \n",
      "\n",
      "Epoch: 20\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07641, iou_score - 0.6262]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.1022, iou_score - 0.555]  \n",
      "\n",
      "Epoch: 21\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07532, iou_score - 0.6297]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1319, iou_score - 0.5181]\n",
      "\n",
      "Epoch: 22\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07136, iou_score - 0.6403]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.3666, iou_score - 0.4532]\n",
      "\n",
      "Epoch: 23\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07053, iou_score - 0.6411]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.73it/s, bce_loss - 0.2707, iou_score - 0.4531]\n",
      "\n",
      "Epoch: 24\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.0717, iou_score - 0.6415] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 0.1016, iou_score - 0.6429] \n",
      "Model saved!\n",
      "\n",
      "Epoch: 25\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.07202, iou_score - 0.6416]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.2187, iou_score - 0.368] \n",
      "Decrease decoder learning rate to 5e-4!\n",
      "\n",
      "Epoch: 26\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.067, iou_score - 0.6585]  \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.80it/s, bce_loss - 0.1166, iou_score - 0.5715]\n",
      "\n",
      "Epoch: 27\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.06538, iou_score - 0.6676]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.09377, iou_score - 0.631] \n",
      "\n",
      "Epoch: 28\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.06397, iou_score - 0.6683]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.2533, iou_score - 0.4415]\n",
      "\n",
      "Epoch: 29\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.06371, iou_score - 0.6736]\n",
      "valid: 100%|██████████| 284/284 [00:25<00:00, 11.13it/s, bce_loss - 0.09225, iou_score - 0.5776]\n",
      "\n",
      "Epoch: 30\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.06291, iou_score - 0.6761]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.69it/s, bce_loss - 0.1351, iou_score - 0.5316]\n",
      "\n",
      "Epoch: 31\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.06193, iou_score - 0.6808]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.72it/s, bce_loss - 0.09378, iou_score - 0.6198]\n",
      "\n",
      "Epoch: 32\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.0605, iou_score - 0.6883] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.72it/s, bce_loss - 0.1261, iou_score - 0.5512]\n",
      "\n",
      "Epoch: 33\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.06083, iou_score - 0.6862]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.68it/s, bce_loss - 0.233, iou_score - 0.4225] \n",
      "\n",
      "Epoch: 34\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.06407, iou_score - 0.6851]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.63it/s, bce_loss - 0.08499, iou_score - 0.6388]\n",
      "\n",
      "Epoch: 35\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05888, iou_score - 0.6978]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.2293, iou_score - 0.3757]\n",
      "\n",
      "Epoch: 36\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05846, iou_score - 0.6968]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.115, iou_score - 0.5861] \n",
      "\n",
      "Epoch: 37\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05776, iou_score - 0.7029]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.1055, iou_score - 0.6024] \n",
      "\n",
      "Epoch: 38\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05729, iou_score - 0.7065]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.08875, iou_score - 0.6474]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 39\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05737, iou_score - 0.7062]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1836, iou_score - 0.5397]\n",
      "\n",
      "Epoch: 40\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05822, iou_score - 0.6997]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 0.1943, iou_score - 0.4862]\n",
      "\n",
      "Epoch: 41\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05548, iou_score - 0.7141]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.09132, iou_score - 0.6228]\n",
      "\n",
      "Epoch: 42\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05636, iou_score - 0.7101]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.1093, iou_score - 0.6227] \n",
      "\n",
      "Epoch: 43\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05483, iou_score - 0.7152]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1529, iou_score - 0.5364]\n",
      "\n",
      "Epoch: 44\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05528, iou_score - 0.7133]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.09774, iou_score - 0.6371]\n",
      "\n",
      "Epoch: 45\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05455, iou_score - 0.7198]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 0.1792, iou_score - 0.4934]\n",
      "\n",
      "Epoch: 46\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05445, iou_score - 0.7155]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.116, iou_score - 0.5979] \n",
      "\n",
      "Epoch: 47\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05436, iou_score - 0.7206]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.80it/s, bce_loss - 0.1151, iou_score - 0.5425] \n",
      "\n",
      "Epoch: 48\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05523, iou_score - 0.7198]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.134, iou_score - 0.5542] \n",
      "\n",
      "Epoch: 49\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05244, iou_score - 0.7304]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.11, iou_score - 0.6115]   \n",
      "\n",
      "Epoch: 50\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.23it/s, bce_loss - 0.05266, iou_score - 0.727] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.115, iou_score - 0.6175] \n",
      "\n",
      "Epoch: 51\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05464, iou_score - 0.714] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1429, iou_score - 0.5485]\n",
      "\n",
      "Epoch: 52\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05225, iou_score - 0.7295]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1347, iou_score - 0.5452]\n",
      "\n",
      "Epoch: 53\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05247, iou_score - 0.7303]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.2175, iou_score - 0.4788]\n",
      "\n",
      "Epoch: 54\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05112, iou_score - 0.7365]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 1.257, iou_score - 0.1031]\n",
      "\n",
      "Epoch: 55\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05098, iou_score - 0.7337]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.1236, iou_score - 0.5825]\n",
      "\n",
      "Epoch: 56\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05095, iou_score - 0.7372]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.1124, iou_score - 0.6054] \n",
      "\n",
      "Epoch: 57\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05019, iou_score - 0.7397]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.139, iou_score - 0.5628] \n",
      "\n",
      "Epoch: 58\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.06024, iou_score - 0.713] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.2133, iou_score - 0.4441]\n",
      "\n",
      "Epoch: 59\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05066, iou_score - 0.7387]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.1081, iou_score - 0.6285] \n",
      "\n",
      "Epoch: 60\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04913, iou_score - 0.7491]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1101, iou_score - 0.6292] \n",
      "\n",
      "Epoch: 61\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04873, iou_score - 0.7525]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1577, iou_score - 0.5858]\n",
      "\n",
      "Epoch: 62\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04876, iou_score - 0.7502]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.09234, iou_score - 0.6489]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 63\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04852, iou_score - 0.7512]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1026, iou_score - 0.6484] \n",
      "\n",
      "Epoch: 64\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.0486, iou_score - 0.7449] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.73it/s, bce_loss - 0.1085, iou_score - 0.5965] \n",
      "\n",
      "Epoch: 65\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04857, iou_score - 0.7496]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.09545, iou_score - 0.6569]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 66\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04845, iou_score - 0.7396]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1228, iou_score - 0.5737]\n",
      "\n",
      "Epoch: 67\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04793, iou_score - 0.7513]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1327, iou_score - 0.6232]\n",
      "\n",
      "Epoch: 68\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04877, iou_score - 0.7471]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.1342, iou_score - 0.5967]\n",
      "\n",
      "Epoch: 69\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04848, iou_score - 0.7488]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1155, iou_score - 0.6149]\n",
      "\n",
      "Epoch: 70\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04705, iou_score - 0.7543]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1176, iou_score - 0.6343]\n",
      "\n",
      "Epoch: 71\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04699, iou_score - 0.7509]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.29, iou_score - 0.4646]  \n",
      "\n",
      "Epoch: 72\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04669, iou_score - 0.7568]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1531, iou_score - 0.5963]\n",
      "\n",
      "Epoch: 73\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04744, iou_score - 0.7557]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.74it/s, bce_loss - 0.1092, iou_score - 0.6287] \n",
      "\n",
      "Epoch: 74\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04685, iou_score - 0.7531]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1055, iou_score - 0.606]  \n",
      "\n",
      "Epoch: 75\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.0468, iou_score - 0.7547] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 0.1087, iou_score - 0.6292] \n",
      "\n",
      "Epoch: 76\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04748, iou_score - 0.7533]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.2493, iou_score - 0.4213]\n",
      "\n",
      "Epoch: 77\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04575, iou_score - 0.7606]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1806, iou_score - 0.5402]\n",
      "\n",
      "Epoch: 78\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04593, iou_score - 0.7613]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.1647, iou_score - 0.5856]\n",
      "\n",
      "Epoch: 79\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04597, iou_score - 0.764] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 0.09994, iou_score - 0.6445]\n",
      "\n",
      "Epoch: 80\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04659, iou_score - 0.76]  \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.1082, iou_score - 0.625]  \n",
      "\n",
      "Epoch: 81\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04533, iou_score - 0.7634]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.128, iou_score - 0.6497] \n",
      "\n",
      "Epoch: 82\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.0457, iou_score - 0.763]  \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.148, iou_score - 0.6066] \n",
      "\n",
      "Epoch: 83\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04532, iou_score - 0.7672]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.1658, iou_score - 0.5612]\n",
      "\n",
      "Epoch: 84\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04474, iou_score - 0.769] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1248, iou_score - 0.6491]\n",
      "\n",
      "Epoch: 85\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04492, iou_score - 0.767] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1639, iou_score - 0.5583]\n",
      "\n",
      "Epoch: 86\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.05202, iou_score - 0.7492]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1767, iou_score - 0.53]  \n",
      "\n",
      "Epoch: 87\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04518, iou_score - 0.757] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 0.2025, iou_score - 0.523] \n",
      "\n",
      "Epoch: 88\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04375, iou_score - 0.7694]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 0.2655, iou_score - 0.3933]\n",
      "\n",
      "Epoch: 89\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04333, iou_score - 0.7722]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.79it/s, bce_loss - 0.1568, iou_score - 0.6011]\n",
      "\n",
      "Epoch: 90\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04293, iou_score - 0.7758]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.76it/s, bce_loss - 0.1013, iou_score - 0.6517] \n",
      "\n",
      "Epoch: 91\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04268, iou_score - 0.7771]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1819, iou_score - 0.54]  \n",
      "\n",
      "Epoch: 92\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04435, iou_score - 0.7677]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1094, iou_score - 0.5786] \n",
      "\n",
      "Epoch: 93\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04306, iou_score - 0.7747]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.80it/s, bce_loss - 0.1327, iou_score - 0.6049]\n",
      "\n",
      "Epoch: 94\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04278, iou_score - 0.7796]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1765, iou_score - 0.5653]\n",
      "\n",
      "Epoch: 95\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04264, iou_score - 0.7783]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.1832, iou_score - 0.5873]\n",
      "\n",
      "Epoch: 96\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04399, iou_score - 0.7637]   \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.1478, iou_score - 0.5785]\n",
      "\n",
      "Epoch: 97\n",
      "train: 100%|██████████| 662/662 [03:23<00:00,  3.25it/s, bce_loss - 0.04635, iou_score - 0.7574]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.78it/s, bce_loss - 0.1109, iou_score - 0.6452] \n",
      "\n",
      "Epoch: 98\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.04263, iou_score - 0.7742]\n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.75it/s, bce_loss - 0.1376, iou_score - 0.6027]\n",
      "\n",
      "Epoch: 99\n",
      "train: 100%|██████████| 662/662 [03:24<00:00,  3.24it/s, bce_loss - 0.0427, iou_score - 0.7793] \n",
      "valid: 100%|██████████| 284/284 [00:24<00:00, 11.77it/s, bce_loss - 0.1631, iou_score - 0.5873]\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "max_score = 0.61\n",
    "\n",
    "'''checkpoint = torch.load(\"../geonet/weights/best/forestnet_chekpoint_v1.2 - 0.613902030818754.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']'''\n",
    "\n",
    "for i in range(0, 100):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        best_model = model\n",
    "        torch.save(best_model, '../geonet/weights/forestnet-c_bce_v1.3 - {}.pth'.format(max_score))\n",
    "        torch.save({\n",
    "            'epoch': i,\n",
    "            'model_state_dict': best_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, '../geonet/weights/forestnet_chekpoint-c_bce_v1.3 - {}.pth'.format(max_score))\n",
    "        print('Model saved!')\n",
    "        \n",
    "    if i == 25:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-4\n",
    "        print('Decrease decoder learning rate to 5e-4!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': i,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, '../forestnet/weights/forestnet_chekpoint_v1.2 - 0.69.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../geonet/weights/forestnet_v1.2 - 0.69.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = utils.flatten_file(\"../../data/_mvp/liski_3857.tif\")\n",
    "Y_tr = raster.get_array_from_tiff(\"../../data/_mvp/liski_forest_mask.tif\")[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(123109938, 4)\n",
      "(123109938,)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.shape)\n",
    "print(Y_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_tr, Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task':'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'histogram_pool_size': 16,\n",
    "    'min_data_in_leaf': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.train(params, lgb_train, num_boost_round=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refit(model, X, Y, decay_rate, chunk = 5000000):\n",
    "    X = utils.flatten_file(X)\n",
    "    Y = raster.get_array_from_tiff(Y)[0].flatten()\n",
    "    steps = np.int(np.floor(X.shape[0] / chunk))\n",
    "    first_step = 0\n",
    "    for i in range(0, steps):\n",
    "        model.refit(X[first_step:(first_step+chunk), :], Y[first_step:(first_step+chunk)], decay_rate)\n",
    "        first_step += chunk\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = lgb.Booster(model_file='../geonet/weights/best/forest_gbm_v0.1.2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = refit(mod, \"../../data/_mvp/liski_3857.tif\", \"../../data/_mvp/liski_forest_mask.tif\", decay_rate=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7fc817640a90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.save_model('../geonet/weights/forest_gbm_v0.1.3.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(img, mas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = raster.get_array_from_tiff(\"../../data/_mvp/vrn_19_coord.tif\")\n",
    "img = np.dstack(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas = raster.get_array_from_tiff(\"../../data/_mvp/vrn_3857_forest_mask.tif\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/DVL/glotovaa/projects/data_science/lib/python3.6/site-packages/albumentations/augmentations/transforms.py:2908: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  \"Using lambda is incompatible with multiprocessing. \"\n"
     ]
    }
   ],
   "source": [
    "testData = dataset.RasterDataset(img, mas, classes=['agro'], tile_size=1024, step=256, preprocessing=get_inference_preprocessing(preprocessing_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testData, batch_size=1, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = torch.load('../geonet/weights/forestnet-c_bce_v1.3 - 0.6568863365000599.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_predict(model, img, test_loader):\n",
    "    ext_x = np.zeros(shape=(img.shape[0], img.shape[1]), dtype=np.float32)\n",
    "    step = 256\n",
    "    tile_size = 1024\n",
    "    xc = round(img.shape[0] / step) + 1\n",
    "    yc = round(img.shape[1] / step) + 1\n",
    "\n",
    "    i = 0\n",
    "    for batch in test_loader:\n",
    "        m = i % xc\n",
    "        j = i // xc\n",
    "        #x_tensor = torch.from_numpy(batch[0]).unsqueeze(0)\n",
    "        pr_mask = model.predict(batch[0].cuda())\n",
    "        pr_mask = (pr_mask.cpu().numpy().round(decimals=2))\n",
    "\n",
    "            \n",
    "        if (step*m+tile_size) > img.shape[0]:\n",
    "            if (step*j+tile_size) > img.shape[1]:\n",
    "                ext_x[(img.shape[0]-tile_size):img.shape[0], (img.shape[1]-tile_size):img.shape[1]] = np.maximum(ext_x[(img.shape[0]-tile_size):img.shape[0], (img.shape[1]-tile_size):img.shape[1]], pr_mask)\n",
    "            else:\n",
    "                ext_x[(img.shape[0]-tile_size):img.shape[0], step*j:(step*j+tile_size)] = np.maximum(ext_x[(img.shape[0]-tile_size):img.shape[0], step*j:(step*j+tile_size)], pr_mask)\n",
    "        elif (step*j+tile_size) > img.shape[1]:\n",
    "            ext_x[step*m:(step*m+tile_size), (img.shape[1]-tile_size):img.shape[1]] = np.maximum(ext_x[step*m:(step*m+tile_size), (img.shape[1]-tile_size):img.shape[1]], pr_mask)\n",
    "        else:\n",
    "            ext_x[step*m:(step*m+tile_size), step*j:(step*j+tile_size)] = np.maximum(ext_x[step*m:(step*m+tile_size), step*j:(step*j+tile_size)], pr_mask)\n",
    "    \n",
    "        i += 1\n",
    "    \n",
    "    return ext_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.dstack(raster.get_array_from_tiff(\"../../data/_mvp/vrn_19_coord.tif\"))\n",
    "ext_x = cnn_predict(forest_model, data, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.get_raster_from_array(ext_x, \"../../data_science/mvp/vrn_19_forest_predicted_v.1.3-c_bce.tif\", \"../../data/_mvp/vrn_19_coord.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_predict(img):\n",
    "    forest_model = torch.load('../geonet/weights/best/forestnet_v1.2 - 0.613902030818754.pth')\n",
    "    image = raster.get_array_from_tiff(img)\n",
    "    cnn_img = np.dstack(image)\n",
    "    testData = dataset.RasterDataset(cnn_img.astype(float), cnn_img[0].astype(float), classes=['agro'], tile_size=1024, step=768, preprocessing=get_inference_preprocessing(preprocessing_fn))\n",
    "    test_loader = DataLoader(testData, batch_size=1, shuffle=False, num_workers=1)\n",
    "    gbm = lgb.Booster(model_file=\"../geonet/weights/forest_gbm_v0.1.3.txt\")\n",
    "    cnn_pred = cnn_predict(forest_model, cnn_img, test_loader)\n",
    "    gbm_pred = gbm.predict(utils.flatten_file(img))\n",
    "    gbm_pred = np.reshape(gbm_pred, image[0].shape)\n",
    "    preds = np.maximum(cnn_pred, gbm_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11109, 11082)\n",
      "(11109, 11082)\n",
      "(11109, 11082)\n"
     ]
    }
   ],
   "source": [
    "d = complex_predict(\"../../data/_mvp/liski_3857.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = lgb.Booster(model_file=\"../geonet/weights/forest_gbm_v0.1.3.txt\")\n",
    "gbm_pred = gbm.predict(utils.flatten_file(\"../../data/_mvp/vrn_train_3857.tif\"))\n",
    "image = raster.get_array_from_tiff(\"../../data/_mvp/vrn_train_3857.tif\")\n",
    "shape=image[0].shape\n",
    "del(image)\n",
    "gbm_pred = np.reshape(gbm_pred, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_x = cnn_predict(forest_model, img, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.maximum(ext_x, gbm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.get_raster_from_array(f, \"../mvp/vrn_forest_predicted_cpx_v2.tif\", \"../../data/_mvp/vrn_train_3857.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(ext_x, f, gbm_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
